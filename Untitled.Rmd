---
title: "Practical Machnie Learning Peer Graded Assinment"
author: "samer"
date: "8/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Exuctive Summary
This project applies practical machine learning to wearable device data to discriminate between different activities and recognize which activity was performed. The approach is applied for the Weight Lifting Exercises dataset to investigate how well an activity was performed by the wearer. The research data defines quality of execution and investigate three aspects that pertain to qualitative activity recognition. In the data six young health participants perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).  The training and validation data sets were imported from the specified link and three predictive models were trained using the data.  The predictive models were applied to the validation set and the results was used to answer the quiz.

## Overview
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit enables collecting pertinent personal activity data. These devices can quantify self-movement. Enthusiasts taking measurements about themselves regularly can improve their health by monitoring patterns in their behavior. Typically, people regularly monitor the quantity of a particular activity, but they seldom quantify the quality or how well they do the activity.

This project uses data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants to discern if the participants perform the activities correctly. The participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

The data consists of a Training data and a Test data (to be used to validate the selected model).

The goal of the project is to predict the manner in which participants did the exercise. This is the “classe” variable in the training set. Any other variables can be used to predict with.

Note: The dataset used in this project is a courtesy of “Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers’ Data Classification of Body Postures and Movements”




``` {r}
library(caret)
```


``` {r}
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
library(corrplot)
library(gbm)
```

## Import and Process Data

### Import Data
``` {r}
trn_url<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
vld_url<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training<-read.csv(trn_url)
validating<-read.csv(vld_url)
dim(training)

```

``` {r}
dim(validating)

```

### Process and Clean Data
remove data with NA values

``` {r}
trnData<-training[, colSums(is.na(training)) == 0]
vldData<-validating[, colSums(is.na(validating)) == 0]
dim(trnData)
```
``` {r}
dim(vldData)
```

remove the first seven variables since they have little impact on the outcome "classe".

``` {r}
trnData<-trnData[,-c(1:7)] 
vldData<-vldData[,-c(1:7)] 
dim(trnData)


```




``` {r}
dim(vldData)
```

### Prepare the datasets for prediction
Preparing the data for prediction by splitting the training data into 70% as train data and 30% as test data. This splitting will server also to compute the out-of-sample errors.

The test data renamed: vldData (validate data) will stay as is and will be used later to test the prodction algorithm on the 20 cases.

```{r}
set.seed(1234) 
inTrain <- createDataPartition(trnData$classe, p = 0.7, list = FALSE)
testData <- trnData[-inTrain, ]
trainData <- trnData[inTrain, ]
dim(trainData)

```

```{r}
dim(testData)

```
###  Remove variables with near-zero variance.

``` {r}
NZV <- nearZeroVar(trainData)
trainData <- trainData[, -NZV]
testData  <- testData[, -NZV]
dim(trainData)
```

``` {r}
dim(testData)
```


This is the last data cleaning.  After this cleaning, only 53 variables remain.

The following correlation plot uses the following parameters (source:CRAN Package ‘corrplot’) “FPC”: the first principal component order. “AOE”: the angular order tl.cex Numeric, for the size of text label (variable names) tl.col The color of text label.

```{r}
cor_mat <- cor(trainData[, -53])
corrplot(cor_mat, order = "FPC", method = "color", type = "upper", 
         tl.cex = 0.8, tl.col = rgb(0, 0, 0))
```

in the next step remove highly correlated variables.

``` {r}
highlyCorrelated = findCorrelation(cor_mat, cutoff=0.75)

```

``` {r}
names(trainData)[highlyCorrelated]
```

## Model building
three algorithms were used to predict the outcome.

 - classification trees
 - random forests
 - Generalized Boosted Model

## Prediction with classification trees

###  obtain the model and use the fancyRpartPlot() function to plot the classification tree as a dendogram.

``` {r, cache=TRUE}
set.seed(12345)
decisionTreeMod1 <- rpart(classe ~ ., data=trainData, method="class")
fancyRpartPlot(decisionTreeMod1)

```

### validate the model “decisionTreeModel” on the testData to find out how well it performs by looking at the accuracy variable.
``` {r}
predictTreeMod1 <- predict(decisionTreeMod1, testData, type = "class")
cmtree <- confusionMatrix(predictTreeMod1, testData$classe)
cmtree
```


### plot matrix results
``` {r}
plot(cmtree$table, col = cmtree$byClass, 
     main = paste("Decision Tree - Accuracy =", round(cmtree$overall['Accuracy'], 4)))
```


The accuracy rate of the model is low: 0.0.7541 and therefore the out-of-sample-error is about 0.2459 which is considerable.

## Prediction with Random Forest
###  determine the model
``` {r, cache=TRUE}
controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
modRF1 <- train(classe ~ ., data=trainData, method="rf", trControl=controlRF)
modRF1$finalModel
```


### validate the model obtained model “modRF1” on the test data to find out how well it performs by looking at the Accuracy variable
``` {r}
predictRF1 <- predict(modRF1, newdata=testData)
cmrf <- confusionMatrix(predictRF1, testData$classe)
cmrf
```



The accuracy rate using the random forest is very high: Accuracy : 0.994 and therefore the out-of-sample-error is equal to 0.006. But it might be due to overfitting.

###  plot the model
``` {r}
plot(modRF1)
```

```{r}
plot(cmrf$table, col = cmrf$byClass, main = paste("Random Forest Confusion Matrix: Accuracy =", round(cmrf$overall['Accuracy'], 4)))
```

## Prediction with Generalized Boosted Regression Models

### determine the model
``` {r, cache=TRUE}
set.seed(12345)
controlGBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
modGBM  <- train(classe ~ ., data=trainData, method = "gbm", trControl = controlGBM, verbose = FALSE)
modGBM$finalModel
```

###  print model summary
``` {r}
 
print(modGBM)
```

###  validate the GBM model
``` {r}
predictGBM <- predict(modGBM, newdata=testData)
cmGBM <- confusionMatrix(predictGBM, testData$classe)
cmGBM
```

The accuracy rate using the random forest is very high: Accuracy : 0.9679 and therefore the out-of-sample-error is equal to 0.0321

## Applying the best model to the validation data
By comparing the accuracy rate values of the three models, it is clear the the ‘Random Forest’ model is the winner. So it will be used on the validation data
``` {r}
Results <- predict(modRF1, newdata=vldData)
Results
```

## Conclusion

Practical machine learning was applied to wearable device data. Data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants was used predict how well the participants performed the exercise. Three models were used (classification trees, random forests, and Generalized Boosted Model). Training data was used to train the models.  The most accurate model was used to predict Test data for validations.

